{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Q-Learning mit OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Übung wird mit einem Jupyter-Notebook angegeben, kann aber auch in Rmarkdown übertragen werden.\n",
    "\n",
    "Ziel ist es, mit dem OpenAI Gym einen \"eigenen\" Agent zu erstellen.\n",
    "\n",
    "Das Beispiel wird als \"Selbstfahrendes\" Taxi gezeigt - es kann aber auch jeder ein \"eigenes\" Gym ausprobieren.\n",
    "\n",
    "## Beispiel: Selbstfahrendes Taxi:\n",
    "\n",
    "Lasst uns eine Simulation von einem selbstfahrenden Taxi entwerfen. Das Hauptziel ist es, in einer vereinfachten Umgebung zu demonstrieren, wie man mit Hilfe von RL-Techniken einen effizienten und sicheren Ansatz zur Lösung dieses Problems entwickeln kann.\n",
    "\n",
    "Die Aufgabe der Smartcab ist es, den Fahrgast an einem Ort abzuholen und an einem anderen Ort abzusetzen. Hier sind ein paar Dinge, um die wir uns mit unserem Smartcab gerne kümmern würden:\n",
    "\n",
    "- Lasst den Beifahrer an der richtigen Stelle zurück.\n",
    "- Zeitersparnis für die Fahrgäste durch minimale Zeitersparnis beim Absetzen der Fahrgäste\n",
    "- Beachten Sie die Sicherheit der Fahrgäste und die Verkehrsregeln.\n",
    "\n",
    "Es gibt verschiedene Aspekte, die hier bei der Modellierung einer RL-Lösung für dieses Problem berücksichtigt werden müssen: Belohnungen, Zustände und Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rewards\n",
    "Da der Agent (der imaginäre Fahrer) belohnungsmotiviert ist und lernen wird, wie man das Taxi durch Trial-Erfahrungen in der Umwelt steuert, müssen wir die Belohnungen und/oder Strafen und deren Höhe entsprechend festlegen. Hier ein paar Punkte, die zu beachten sind:\n",
    "\n",
    "- Der Agent sollte eine hohe positive Belohnung für einen erfolgreichen Dropoff erhalten, da dieses Verhalten sehr erwünscht ist.\n",
    "- Der Agent sollte bestraft werden, wenn er versucht, einen Passagier an falschen Orten abzusetzen.\n",
    "- Der Agent sollte eine leichte negative Belohnung dafür erhalten, dass er es nach jedem Zeitschritt nicht bis zum Ziel geschafft hat. \"Leicht\" negativ, weil wir es vorziehen würden, dass unser Agent zu spät kommt, anstatt falsche Schritte zu unternehmen und zu versuchen, das Ziel so schnell wie möglich zu erreichen.\n",
    "\n",
    "\n",
    "# 2. State Space\n",
    "Beim Reinforcement Learning begegnet der Agent einem Zustand und ergreift dann Maßnahmen entsprechend dem Zustand, in dem er sich befindet.\n",
    "\n",
    "Der State Space ist die Gesamtheit aller möglichen Situationen, in denen unser Taxi leben könnte. Der Zustand sollte nützliche Informationen enthalten, die der Agent benötigt, um die richtigen Maßnahmen zu ergreifen.\n",
    "\n",
    "Nehmen wir an, wir haben einen Trainingsbereich für unser Smartcab, in dem wir ihm beibringen, Menschen auf einem Parkplatz zu vier verschiedenen Orten (R, G, Y, B) zu transportieren:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![taxi_env](taxi_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nehmen wir an, Smartcab ist das einzige Fahrzeug auf diesem Parkplatz. Wir können den Parkplatz in ein 5x5 Raster aufteilen, was uns 25 mögliche Taxistandorte gibt. Diese 25 Standorte sind ein Teil unseres Staatsraums. Beachten Sie, dass der aktuelle Standortzustand unseres Taxis koordiniert ist (3, 1).\n",
    "\n",
    "Sie werden auch feststellen, dass es vier (4) Orte gibt, an denen wir einen Passagier abholen und absetzen können: R, G, Y, B oder ´[(0,0), (0,4), (4,0), (4,0), (4,3)]´ in Koordinaten (Reihe, Spalte). Unser illustrierter Passagier ist in Position Y und möchte zu Position **R** gehen.\n",
    "\n",
    "Wenn wir auch einen (1) zusätzlichen Fahrgastzustand innerhalb des Taxis berücksichtigen, können wir alle Kombinationen von Fahrgast- und Zielorten nehmen, um zu einer Gesamtzahl von Zuständen für unsere Taxiumgebung zu gelangen; es gibt vier (4) Ziele und fünf (4 + 1) Fahrgastziele.\n",
    "\n",
    "So hat unsere Taxiumgebung $5×5×5×5×4=500$ mögliche Zustände.\n",
    "\n",
    "Der Agent trifft auf einen der 500 Zustände und er ergreift eine Aktion. Die Aktion in unserem Fall kann sein, sich in eine Richtung zu bewegen oder sich zu entscheiden, einen Fahrgast abzuholen oder abzusetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Space \n",
    "Mit anderen Worten, wir haben sechs mögliche Aktionen:\n",
    "\n",
    "1. `south`\n",
    "2. `north` \n",
    "3. `east` \n",
    "4. `west` \n",
    "5. `pickup` \n",
    "6. `dropoff` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist der Action Space: der Satz aller Aktionen, die unser Agent in einem bestimmten Zustand ausführen kann.\n",
    "\n",
    "Sie werden in der obigen Abbildung feststellen, dass das Taxi in bestimmten Zuständen aufgrund von Mauern bestimmte Aktionen nicht ausführen kann. Im Code der Umgebung geben wir einfach eine -1 Strafe für jeden Mauerstoß und das Taxi bewegt sich nirgendwo hin. Dies wird nur Strafen mit sich bringen, die das Taxi dazu bringen, eine Umgehung der Mauer in Betracht zu ziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup der Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Kern-Schnittstelle des Gym ist env, das ist die Unified Environment Interface. Nachfolgend sind die env-Methoden aufgeführt, die uns sehr hilfreich sein könnten:\n",
    "\n",
    "- `env.reset`: Setzt die Umgebung zurück und gibt einen zufälligen Ausgangszustand zurück.\n",
    "- `env.step(action)`: Schieben Sie die Umgebung um einen Zeitschritt nach vorne. Retouren\n",
    "  - observation: Umweltbeobachtungen\n",
    "  - reward: Ob Ihre Aktion nützlich war oder nicht.\n",
    "  - done: Zeigt an, ob wir einen Passagier, auch eine Episode genannt, erfolgreich abgeholt und abgesetzt haben.\n",
    "  - info: Zusätzliche Informationen wie Performance und Latenzzeiten für Debuggingzwecke\n",
    "- `env.render`: Stellt einen Rahmen der Umgebung dar (hilfreich bei der Visualisierung der Umgebung)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328] # Reward Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lösung ohne Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 3830\n",
      "Penalties incurred: 1257\n",
      "Rewards earned : 20\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n",
    "print(\"Rewards earned : {}\".format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3830"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rflearning_reward = reward / epochs\n",
    "no_rflearning_epochs = epochs\n",
    "no_rflearning_penalties = penalties / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "Timestep: 44\n",
      "State: 388\n",
      "Action: 0\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-adcf58a03b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-adcf58a03b7e>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 : Implementierung von Q-Learning in python (5 Punkte)\n",
    "\n",
    "**Tipps:**\n",
    "Versuche mit einem Random-Agent-Template zu starten und wenn möglich - objektorientiert vorzugehen. (Random-Agent Klasse: https://github.com/openai/gym/blob/master/examples/agents/random_agent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 16s, sys: 17.5 s, total: 1min 33s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Train the Agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameter festlegen\n",
    "alpha = 0.2\n",
    "gamma = 0.5\n",
    "epsilon = 0.2\n",
    "\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Neue Spaces entdecken\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Von gelernten Erfahrungen entscheiden\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) # Neuen Wert anhand Q-Learning Formel berechnen\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.98925781,  -1.95703125,  -1.98925781,  -1.97851563,\n",
       "       -10.97851563, -10.97851563])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance des Agenten testen, nach dem Q-Learning angewandt wurde\n",
    "Ich führe diesen Test über 100 Epochen durch und berechne dann die durchschnittliche Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnis nach 100 Episoden:\n",
      "Durchschnittliche Zeit pro Episode: 13.31\n",
      "Durchschnittliche Strafen pro Episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Test Performance\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_rewards += reward\n",
    "\n",
    "print(f\"Ergebnis nach {episodes} Episoden:\")\n",
    "print(f\"Durchschnittliche Zeit pro Episode: {total_epochs / episodes}\")\n",
    "print(f\"Durchschnittliche Strafen pro Episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Vergleich von Q-Learning (10 Punkte)\n",
    "\n",
    "\n",
    "Wir bewerten unsere Agenten anhand der folgenden Kennzahlen,\n",
    "\n",
    "- **Durchschnittliche Anzahl der Strafen pro Episode**: Je kleiner die Zahl, desto besser die Leistung unseres Agenten. Im Idealfall möchten wir, dass diese Kennzahl Null oder sehr nahe bei Null liegt.\n",
    "- **Durchschnittliche Anzahl der Zeitschritte pro Fahrt**: Wir wollen auch eine kleine Anzahl von Zeitschritten pro Episode, da wir möchten, dass unser Agent minimale Schritte (d.h. den kürzesten Weg) unternimmt, um das Ziel zu erreichen.\n",
    "- **Durchschnittliche Belohnungen pro Zug**: Je größer die Belohnung, desto besser ist es, dass der Agent das Richtige tut. Deshalb ist die Entscheidung über die Belohnung ein wichtiger Teil des Verstärkungslernens. In unserem Fall, da sowohl Zeitschritte als auch Strafen negativ belohnt werden, würde eine höhere durchschnittliche Belohnung bedeuten, dass der Agent das Ziel so schnell wie möglich mit den geringsten Strafen erreicht\".\n",
    "\n",
    "\n",
    "Erstellen Sie eine Tabelle und vergleichen Sie Q-Learning mit dem \"simplen\" Ansatz von oben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_reward = reward / episodes\n",
    "q_learning_epochs = total_epochs / episodes\n",
    "q_learning_penalties = total_penalties / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = {\"Modell\" : [\"No RF Learning\", \"Q-Learning\"],\n",
    "                     \"Avg Rewards\" : [no_rflearning_reward, q_learning_reward],\n",
    "                     \"Avg Penalties\" : [no_rflearning_penalties, q_learning_penalties],\n",
    "                    \"Avg Epochs\" : [no_rflearning_epochs, q_learning_epochs]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Q-Learning Algorithmus schneidet deutlich besser ab als die Lösung ohne Reinforcement Learning. Das Q-Learning Modell schafft es nach dem der Agent 100000 Episoden trainiert wurde, die Strafen komplett zu umgehen. Auch der durchsnchittliche Reward pro Epoche um ein Vielfaches höher ist. Bei dem einen Durchlauf ohne RL hat der Agent 3830 Schritte benötigt, um an Ziel zu kommen. Beim Q-Learning waren es im Schnitt dann nur mehr etwas mehr als 13. Dies zeigt deutlich, dass durch Q-Learning eine deutlich bessere Performance erzielt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modell</th>\n",
       "      <th>Avg Rewards</th>\n",
       "      <th>Avg Penalties</th>\n",
       "      <th>Avg Epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No RF Learning</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.328198</td>\n",
       "      <td>3830.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Q-Learning</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Modell  Avg Rewards  Avg Penalties  Avg Epochs\n",
       "0  No RF Learning     0.005222       0.328198     3830.00\n",
       "1      Q-Learning     0.200000       0.000000       13.31"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Hyperparameter (10 Punkte)\n",
    "\n",
    "Die Werte von `alpha`, `gamma` und `epsilon` basierten hauptsächlich auf Intuition und etwas \"Hit and Trial\", aber es gibt bessere Möglichkeiten, gute Werte zu finden.\n",
    "\n",
    "Im Idealfall sollten alle drei im Laufe der Zeit abnehmen, denn wenn der Agent weiter lernt, baut er tatsächlich widerstandsfähigere Vorgänger auf;\n",
    "\n",
    "- *α*: (die Lernrate) sollte abnehmen, da Sie immer mehr an einer immer größeren Wissensbasis gewinnen.\n",
    "- *γ*: Wenn Sie der Deadline immer näher kommen, sollte Ihre Präferenz für eine kurzfristige Belohnung steigen, da Sie nicht lange genug dabei sein werden, um die langfristige Belohnung zu erhalten, was bedeutet, dass Ihr Gamma sinken sollte.\n",
    "- *ϵ*: Während wir unsere Strategie entwickeln, haben wir weniger Bedarf an Exploration und mehr Ausbeutung, um mehr Nutzen aus unserer Policy zu ziehen, so dass mit zunehmender Anzahl der Versuche epsilon abnehmen sollte.\n",
    "\n",
    "Wie können die Hyperparameter \"gesucht\" werden?\n",
    "Versuche mindestens eine Suchstrategie und gib die Hyperparameter an. Wie viele Iterationen benötigt der \"minimale\" Algorithmus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich habe mich bei der Hyperparameteroptimierung für eine Random Search Methode entschieden. Bei der 50 mal random aus den untenaufgeführten Listen den Parameter wähle und dann prüfe mit welchen Parametern das beste Ergebnis erzielt wird. Ich habe bewusst bei den Parametern den Wert 0.1 ausgelassen, da ansonsten mein Rechner \"ewig\" rechnet. Ich teste für jedes Parametertrio die Performance über 100 Episoden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "possibilities = [0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100\n",
      "Training finished.\n",
      "\n",
      "49\n",
      "FINISHED!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# Leere Listen um Werte einzufügen\n",
    "#all_epochs = []\n",
    "#all_penalties = []\n",
    "alpha_list = []\n",
    "gamma_list = []\n",
    "epsilon_list = []\n",
    "rewards_list = []\n",
    "epochs_list = []\n",
    "penalties_list = []\n",
    "    \n",
    "for x in range(50):\n",
    "    alpha = random.choice(possibilities)\n",
    "    gamma = random.choice(possibilities)\n",
    "    epsilon = random.choice(possibilities)\n",
    "    \n",
    "    for i in range(1, 101):\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "    \n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() \n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) \n",
    "\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "        \n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Iteration beendet\\n\")\n",
    "    epochs_list.append(epochs)            \n",
    "    penalties_list.append(penalties)        \n",
    "    rewards_list.append(reward)\n",
    "    epsilon_list.append(epsilon)\n",
    "    alpha_list.append(alpha)\n",
    "    gamma_list.append(gamma)\n",
    "    print(x)\n",
    "    x += 1\n",
    "print(\" JUHU FINISHED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_df = pd.DataFrame()\n",
    "\n",
    "parameter_df[\"Alphas\"] = alpha_list\n",
    "parameter_df[\"Epsilon\"] = epsilon_list\n",
    "parameter_df[\"Gamma\"] = gamma_list\n",
    "parameter_df[\"Rewards\"] = rewards_list\n",
    "parameter_df[\"Epochs\"] = epochs_list\n",
    "parameter_df[\"Penalties\"] = penalties_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_df[\"Reward per Epoch\"] = parameter_df[\"Rewards\"] / parameter_df[\"Epochs\"]\n",
    "parameter_df[\"Penalty per Epoch\"] = parameter_df[\"Penalties\"] / parameter_df[\"Epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alphas</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Rewards</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Penalties</th>\n",
       "      <th>Reward per Epoch</th>\n",
       "      <th>Penalty per Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alphas  Epsilon  Gamma  Rewards  Epochs  Penalties  Reward per Epoch  \\\n",
       "35     0.5     0.15    0.3       20      11          0          1.818182   \n",
       "6      0.2     0.50    0.6       20      12          1          1.666667   \n",
       "45     0.7     0.15    0.9       20      13          0          1.538462   \n",
       "26     0.8     0.15    0.5       20      14          0          1.428571   \n",
       "12     0.8     0.15    1.0       20      14          0          1.428571   \n",
       "\n",
       "    Penalty per Epoch  \n",
       "35           0.000000  \n",
       "6            0.083333  \n",
       "45           0.000000  \n",
       "26           0.000000  \n",
       "12           0.000000  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_df.sort_values(by=[\"Reward per Epoch\"], ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alphas</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Rewards</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Penalties</th>\n",
       "      <th>Reward per Epoch</th>\n",
       "      <th>Penalty per Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.03125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alphas  Epsilon  Gamma  Rewards  Epochs  Penalties  Reward per Epoch  \\\n",
       "45     0.7     0.15    0.9       20      13          0          1.538462   \n",
       "35     0.5     0.15    0.3       20      11          0          1.818182   \n",
       "26     0.8     0.15    0.5       20      14          0          1.428571   \n",
       "12     0.8     0.15    1.0       20      14          0          1.428571   \n",
       "14     0.7     0.20    0.4       20      64          2          0.312500   \n",
       "\n",
       "    Penalty per Epoch  \n",
       "45            0.00000  \n",
       "35            0.00000  \n",
       "26            0.00000  \n",
       "12            0.00000  \n",
       "14            0.03125  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_df.sort_values(by=[\"Penalty per Epoch\"], ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis nach den 50 Iterationen und unterschiedlichen Parametern zeigt, dass die Modelle mit geringem Epsilon mit 0,15 am besten abschneiden, sowohl bezüglich zu erwartendem Reward als auch bzgl. der zu erwartenden Strafe. Das beste Modell ist bei einem Alpha von 0.5, einem Epsilon von 0,15 und einem Gammawert von 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
